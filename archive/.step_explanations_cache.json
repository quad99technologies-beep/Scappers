{
  "D:\\quad99\\Scappers\\2. Malaysia\\scripts\\03_Consolidate_Results.py": {
    "hash": "b5686197194674f249d61d2323f58d3f",
    "explanation": "### Explanation of the Python Script: `03_Consolidate_Results.py`\n\n#### 1. Main Purpose and Functionality\nThe primary purpose of this script is to consolidate product details from a CSV file (`quest3_product_details.csv`) into a standardized format and save the cleaned data into a new CSV file (`consolidated_products.csv`). This process ensures that the product details are uniform and ready for further processing in subsequent scripts.\n\n#### 2. Key Steps and Workflow\n- **Environment Setup**: The script begins by loading environment variables from a `.env` file to determine file paths for input and output.\n- **File Path Configuration**: It sets up paths for the input file (`quest3_product_details.csv`) and the output file (`consolidated_products.csv`).\n- **File Existence Check**: Before proceeding, it checks if the input file exists. If not, it prompts the user to run a previous script to generate the required file.\n- **Data Reading**: The script reads the input CSV file into a Pandas DataFrame.\n- **Column Validation**: It checks for the presence of required columns (`Registration No`, `Product Name`, `Holder`). If any are missing, it raises an error.\n- **Data Standardization**: The script renames the columns to a standardized format expected by later processing scripts.\n- **Data Cleaning**: It removes rows where either `Product Name` or `Holder` is missing and also eliminates duplicate entries based on the `Registration No / Notification No`.\n- **File Saving**: Finally, it saves the cleaned and consolidated DataFrame to the specified output CSV file.\n\n#### 3. Important Functions and Their Roles\n- **`load_env_file()`**: Loads environment variables from a `.env` file, allowing for dynamic configuration of file paths.\n- **`pd.read_csv()`**: Reads the input CSV file into a DataFrame, enabling data manipulation.\n- **DataFrame operations**: Various operations like checking for missing columns, renaming columns, filtering out rows with missing values, and dropping duplicates are performed using Pandas DataFrame methods.\n- **`to_csv()`**: Saves the cleaned DataFrame to a new CSV file.\n\n#### 4. Input/Output Expectations\n- **Input**: The script expects an input CSV file named `quest3_product_details.csv` containing at least the columns `Registration No`, `Product Name`, and `Holder`.\n- **Output**: The output is a CSV file named `consolidated_products.csv`, which contains:\n  - Standardized column names: `Registration No / Notification No`, `Product Name`, and `Holder`.\n  - Only rows with non-empty `Product Name` and `Holder`.\n  - No duplicate entries based on `Registration No / Notification No`.\n\nThis script is an essential step in the data processing pipeline, ensuring that product details are clean and standardized for further analysis or reporting.",
    "timestamp": "2025-12-26T17:08:14.397935"
  },
  "D:\\quad99\\Scappers\\1. CanadaQuebec\\Script\\05_extract_annexe_v.py": {
    "file_mtime": 1766673428.2977512,
    "explanation": "**TLDR:** This Python script extracts specific data from a PDF document (Annexe V), processes it, and saves the results in a CSV format while handling various data cleaning and validation tasks.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script is designed to extract information from a PDF file containing Annexe V data, which includes product details such as generic names, prices, and manufacturer information. It processes the data to ensure accuracy and saves it in a structured CSV format for further analysis.\n\n2. **Key steps and workflow:**\n   - The script begins by setting up logging and configuration parameters, including paths for input and output files.\n   - It attempts to open the specified PDF file and processes each page starting from a defined page index.\n   - For each line of text in the PDF, it identifies headers, product lines, and data rows using regular expressions and context detection.\n   - It extracts relevant fields, such as product group, manufacturer, format, and prices, while handling potential errors and inconsistencies in the data.\n   - Finally, it writes the extracted data to a CSV file and performs post-processing to clean up any erroneous footer values.\n\n3. **Important functions and their roles:**\n   - `extract_annexe_v()`: The main function that orchestrates the extraction process, logging, and error handling.\n   - `page_to_lines()`: Converts PDF page content into structured lines of text for easier processing.\n   - `is_generic_header()`, `is_form_line()`: Functions to identify specific types of lines in the PDF, helping to categorize the data.\n   - `parse_line_format_cost()`: Extracts format and cost information from the product lines.\n   - `clean_footer_values_from_csv()`: Cleans the output CSV by removing rows that contain footer date values.\n\n4. **Input/output expectations:**\n   - **Input:** The script expects a PDF file named \"annexe_v.pdf\" located in a specified input directory. The PDF should contain structured data relevant to Annexe V.\n   - **Output:** The script generates a CSV file named \"annexe_v_extracted.csv\" in a designated output directory, containing structured data with columns for generic name, currency, prices, and other product details. It also creates a log file documenting the extraction process and any errors encountered.",
    "cache_timestamp": "2025-12-26T18:48:57.644945",
    "file_mtime_str": "2025-12-25T20:07:08.297751"
  },
  "D:\\quad99\\Scappers\\3. Argentina\\script\\01_getCompanyList.py": {
    "file_mtime": 1766729090.5138557,
    "explanation": "**TLDR:** This Python script logs into the AlfaBeta website, retrieves a list of companies from a specific index, handles pagination, and saves the results to a CSV file.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script is designed to automate the process of logging into the AlfaBeta website, submitting a blank search form to retrieve a list of companies, navigating through the paginated results, and saving the extracted company names into a CSV file.\n\n2. **Key steps and workflow:**\n   - It begins by loading environment variables, including user credentials.\n   - It sets up a Selenium WebDriver to interact with the web page, optionally using a proxy for anonymity.\n   - The script logs into the AlfaBeta site and submits a blank search form.\n   - It extracts company names from the resulting pages, handling pagination until no more pages are available.\n   - Finally, it writes the collected company names to a CSV file.\n\n3. **Important functions and their roles:**\n   - `load_env_file()`: Loads environment variables from a `.env` file for configuration.\n   - `setup_driver()`: Configures and initializes the Selenium WebDriver, including proxy settings if provided.\n   - `ensure_logged_in()`: Checks if the user is logged in and performs the login if not.\n   - `submit_blank_companies()`: Submits the blank search form to retrieve company listings.\n   - `extract_companies_page()`: Extracts company names from the current page.\n   - `go_next()`: Navigates to the next page of results if available.\n   - `main()`: Orchestrates the overall flow of the script.\n\n4. **Input/output expectations:**\n   - **Input:** The script requires user credentials (username and password) to log in, which can be set in a `.env` file or directly in the code. It may also use a list of proxy servers from a file.\n   - **Output:** The script outputs a CSV file named `Companylist.csv` in the `./Input` directory, containing a header and a sorted list of unique company names extracted from the AlfaBeta website.",
    "cache_timestamp": "2025-12-26T19:41:07.208754",
    "file_mtime_str": "2025-12-26T11:34:50.513856"
  },
  "D:\\quad99\\Scappers\\2. Malaysia\\scripts\\05_Generate_PCID_Mapped.py": {
    "file_mtime": 1766742477.4220107,
    "explanation": "**TLDR:** This Python script generates a PCID-mapped report for Malaysian drug products by merging data from consolidated product and pricing files, applying mappings, and producing an output file with relevant details.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script's primary purpose is to create a \"Malaysia_PCID Mapped\" report by consolidating information from two CSV files: one containing product details and another with pricing information. It aligns the output to a specified structure, ensuring that missing fields are appropriately handled.\n\n2. **Key steps and workflow:**\n   - The script begins by loading necessary environment variables and defining the final output structure.\n   - It then defines functions to normalize registration numbers, load data from input files, and convert string values to floats.\n   - The main function orchestrates the loading of data, merges the datasets based on a common key (registration number), and builds the final report.\n   - The report is split into mapped and not mapped records, which are saved to separate output files.\n   - A comprehensive summary report is generated to provide insights into the data processing results.\n\n3. **Important functions and their roles:**\n   - `norm_regno(x)`: Normalizes registration numbers by stripping whitespace and converting to uppercase.\n   - `load_pcid_mapping(pcid_path)`: Loads and normalizes PCID mapping data from a CSV file.\n   - `load_inputs(consolidated_path, prices_path)`: Loads and prepares the consolidated product and pricing data, ensuring column names match expected formats.\n   - `build_report(cons, prices, pcid_mapping, fully_reimbursable)`: Merges the dataframes and constructs the final report with the required columns.\n   - `generate_final_report(...)`: Creates a detailed report summarizing the data coverage and any issues encountered during processing.\n\n4. **Input/output expectations:**\n   - **Inputs:** The script expects paths to three CSV files: `consolidated_products.csv`, `malaysia_drug_prices_view_all.csv`, and `malaysia_fully_reimbursable_drugs.csv`. It also requires a PCID mapping file.\n   - **Outputs:** The script generates two output files: one for mapped records (`malaysia_pcid_mapped.csv`) and another for records that could not be mapped (`malaysia_pcid_not_mapped.csv`). Additionally, a text report summarizing the data processing results is created.",
    "cache_timestamp": "2025-12-26T22:32:44.215939",
    "file_mtime_str": "2025-12-26T15:17:57.422011"
  },
  "D:\\quad99\\Scappers\\1. CanadaQuebec\\Script\\01_split_pdf_into_annexes.py": {
    "file_mtime": 1766740170.6842911,
    "explanation": "**TLDR:** This Python script extracts specific sections (Annexes IV.1, IV.2, and V) from a PDF document and saves them as separate PDF files while generating a metadata index.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script is designed to extract specific sections from a PDF document related to pharmaceutical data, specifically the sections labeled \"ANNEXE IV.1\", \"ANNEXE IV.2\", and \"ANNEXE V\". It saves these sections as individual PDF files and creates a JSON metadata file that documents the extraction process.\n\n2. **Key steps and workflow:**\n   - The script begins by importing necessary libraries and defining utility functions.\n   - It sets up paths for input and output directories, attempting to load configuration settings.\n   - It reads the input PDF and identifies the starting and ending pages for each annex section.\n   - Each annex is extracted and saved as a separate PDF file.\n   - Finally, it creates a JSON file containing metadata about the extraction process, including the paths to the extracted files.\n\n3. **Important functions and their roles:**\n   - `strip_accents(text)`: Removes accents from characters for case-insensitive matching.\n   - `extract_page_text(reader, page_idx)`: Extracts and cleans text from a specified page of the PDF.\n   - `find_last_annexe_v_page(reader)`: Locates the last page containing \"ANNEXE V\".\n   - `find_first_legend_after(reader, start_page)`: Finds the first page with \"LÃ‰GENDE\" after a specified starting page.\n   - `find_annexe_iv1_page(reader)`, `find_first_annexe_iv2_page(reader, start_after)`, `find_first_annexe_v_page(reader, start_after)`: Locate the starting pages for each annex section.\n   - `extract_pdf_section(reader, start_page, end_page, output_filename)`: Extracts pages from the PDF and saves them to a new file.\n\n4. **Input/output expectations:**\n   - **Input:** The script expects a PDF file named \"liste-med.pdf\" to be present in the specified input directory. If not found, it searches for any PDF in the input directory.\n   - **Output:** The script generates three PDF files named \"annexe_iv1.pdf\", \"annexe_iv2.pdf\", and \"annexe_v.pdf\" in the output directory, along with a JSON file named \"index.json\" that contains metadata about the extraction process, including the paths to the output files and the page ranges for each annex.",
    "cache_timestamp": "2025-12-29T19:33:37.537080",
    "file_mtime_str": "2025-12-26T14:39:30.684291"
  },
  "D:\\quad99\\Scappers\\1. CanadaQuebec\\Script\\06_merge_all_annexes.py": {
    "file_mtime": 1766742477.4198806,
    "explanation": "**TLDR:** This Python script merges CSV files from three annexes into a single CSV file, ensuring consistent columns and logging the process.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script's primary purpose is to merge CSV outputs from three annexes (IV.1, IV.2, and V) into a single final CSV file. It ensures that the merged data maintains a consistent column structure and fills in any missing columns with predefined static values.\n\n2. **Key steps and workflow:**\n   - The script begins by importing necessary modules and setting up paths for input and output directories.\n   - It defines a standard column order that all annexes should conform to.\n   - The `read_csv_with_columns` function reads each annex's CSV file, normalizes the column names, and fills in static values where necessary.\n   - The `merge_all_annexes` function orchestrates the reading of each annex, compiles the data, and writes the merged output to a new CSV file.\n   - It also logs the process, including the number of rows read from each annex and any errors encountered.\n\n3. **Important functions and their roles:**\n   - `read_csv_with_columns(file_path, annexe_name)`: Reads a CSV file, normalizes its columns to match the standard format, and returns the data as a list of dictionaries.\n   - `merge_all_annexes()`: Manages the overall merging process, calls the reading function for each annex, compiles the results, writes the final CSV, and logs the operation.\n   - `main()`: The entry point of the script, which initiates the merging process and handles the output display.\n\n4. **Input/output expectations:**\n   - **Input:** The script expects three CSV files located in a specified output directory, each corresponding to an annex (IV.1, IV.2, V). The files must have varying column names that the script will normalize.\n   - **Output:** The script generates a merged CSV file named according to a date format (e.g., `canadaquebecreport_ddmmyyyy.csv`) in the output directory. It also logs the process in a specified log file and copies the final report to a central output directory. The output includes a summary of the merging process, including the total number of rows merged and the count from each annex.",
    "cache_timestamp": "2025-12-30T11:19:14.430907",
    "file_mtime_str": "2025-12-26T15:17:57.419881"
  },
  "D:\\quad99\\Scappers\\scripts\\Malaysia\\03_Consolidate_Results.py": {
    "file_mtime": 1767278699.0257025,
    "explanation": "**TLDR:** This script reads product details from a CSV file, standardizes the column names, removes invalid entries, and saves the cleaned data into a new consolidated CSV file.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script consolidates product details from a CSV file named `quest3_product_details.csv`, standardizes the column names to match a specific format, removes any rows with missing critical information, and saves the cleaned data into a new file called `consolidated_products.csv`.\n\n2. **Key steps and workflow:**\n   - It begins by loading environment variables to determine file paths.\n   - It checks if the input file exists and reads its contents into a DataFrame.\n   - The script verifies that required columns are present, renames them for consistency, and filters out rows with missing product names or holders.\n   - Finally, it removes duplicate entries based on the registration number and saves the cleaned DataFrame to a new CSV file.\n\n3. **Important functions and their roles:**\n   - `load_env_file()`: Loads environment variables from a `.env` file to configure file paths.\n   - `pd.read_csv()`: Reads the input CSV file into a DataFrame for processing.\n   - `DataFrame.drop_duplicates()`: Removes duplicate rows based on specified columns.\n   - `DataFrame.to_csv()`: Saves the cleaned DataFrame to a new CSV file.\n\n4. **Input/output expectations:**\n   - **Input:** The script expects an input CSV file named `quest3_product_details.csv` located in a specified output directory. This file must contain columns \"Registration No\", \"Product Name\", and \"Holder\".\n   - **Output:** The output is a new CSV file named `consolidated_products.csv` that contains standardized product details, free from missing values in critical fields and duplicates.",
    "cache_timestamp": "2026-01-01T22:08:27.148806",
    "file_mtime_str": "2026-01-01T20:14:59.025702"
  },
  "D:\\quad99\\Scappers\\scripts\\Malaysia\\05_Generate_PCID_Mapped.py": {
    "file_mtime": 1767279144.2792442,
    "explanation": "**TLDR:** This Python script generates a comprehensive report mapping Malaysian pharmaceutical products to their respective PCID codes, consolidating data from multiple CSV files and producing output files for both mapped and unmapped records.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script's primary function is to create a \"Malaysia_PCID Mapped\" report by merging data from a consolidated product list and a drug pricing file. It aligns the output to a specified structure and handles missing data appropriately, ensuring that the report meets the required format for further analysis.\n\n2. **Key steps and workflow:**\n   - The script begins by loading environment variables and defining the expected output structure.\n   - It defines functions to normalize registration numbers, load data from CSV files, convert string values to floats, and build the final report.\n   - The `main` function orchestrates the workflow: it parses command-line arguments, loads input files, and calls the report-building function.\n   - Finally, it saves the mapped and unmapped records into separate output files and generates a comprehensive report summarizing the data coverage and quality.\n\n3. **Important functions and their roles:**\n   - `norm_regno(x)`: Normalizes registration numbers by stripping whitespace and converting to uppercase.\n   - `load_pcid_mapping(pcid_path)`: Loads PCID mapping data from a CSV file and normalizes the `LOCAL_PACK_CODE`.\n   - `load_inputs(consolidated_path, prices_path)`: Loads and prepares the consolidated product and pricing data, ensuring the columns are correctly named and normalized.\n   - `to_float(series)`: Safely converts string values to floats, handling empty strings and formatting issues.\n   - `build_report(cons, prices, pcid_mapping, fully_reimbursable)`: Merges the dataframes, processes the necessary calculations, and constructs the final output dataframe with the required columns.\n\n4. **Input/output expectations:**\n   - **Inputs:** The script expects three CSV files: `consolidated_products.csv`, `malaysia_drug_prices_view_all.csv`, and `malaysia_fully_reimbursable_drugs.csv`. It also requires a PCID mapping file.\n   - **Outputs:** The script generates two output files: one for mapped records (`Malaysia_PCID_Mapped_generated.xlsx` or `.csv`) and another for unmapped records (`malaysia_pcid_not_mapped.csv`). Additionally, it produces a comprehensive text report summarizing the data coverage and quality metrics.",
    "cache_timestamp": "2026-01-01T22:09:05.946703",
    "file_mtime_str": "2026-01-01T20:22:24.279244"
  },
  "D:\\quad99\\Scappers\\scripts\\CanadaQuebec\\05_extract_annexe_v.py": {
    "file_mtime": 1767283307.8072784,
    "explanation": "**TLDR:** This Python script extracts and processes data from a specified PDF file (Annexe V), converting it into a structured CSV format while applying various data cleaning and validation techniques.\n\n**Full Explanation:**\n\n1. **Main purpose and functionality:**\n   The script is designed to extract specific data from a PDF document (Annexe V), which contains pharmaceutical product information. It processes the data to ensure it is clean and structured, then outputs it to a CSV file. The script includes improvements for error handling, logging, and data validation.\n\n2. **Key steps and workflow:**\n   - The script begins by importing necessary libraries and defining utility functions for text cleaning and CSV handling.\n   - It loads configuration settings such as input/output directories and file names.\n   - It attempts to locate the input PDF file, falling back to alternative paths if necessary.\n   - The script opens the PDF file and iterates through its pages, extracting relevant data based on specific patterns (e.g., generic names, prices, and product details).\n   - It handles potential continuation lines and organizes the extracted data into a structured format.\n   - Finally, it writes the processed data to a CSV file and performs post-processing to remove any erroneous footer values.\n\n3. **Important functions and their roles:**\n   - `extract_annexe_v()`: The main function that orchestrates the extraction process, handles logging, and manages errors.\n   - `page_to_lines()`: Extracts words from a PDF page and organizes them into lines for further processing.\n   - `parse_line_format_cost()`: Parses lines to extract format, cost, and unit price information.\n   - `clean_footer_values_from_csv()`: Cleans up the output CSV by removing rows that contain footer date values.\n\n4. **Input/output expectations:**\n   - **Input:** The script expects a PDF file containing pharmaceutical data, specified by the configuration settings. It also requires certain environment variables for configurable parameters like maximum rows to extract.\n   - **Output:** The output is a CSV file containing structured data with columns for generic name, currency, prices, product group, and other relevant details. Additionally, a log file is generated to record the extraction process and any errors encountered.",
    "cache_timestamp": "2026-01-01T23:10:17.949777",
    "file_mtime_str": "2026-01-01T21:31:47.807278"
  }
}