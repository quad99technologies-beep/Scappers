# Scraper Platform - Repository Structure

## ğŸ“‚ Directory Layout

```
Scrappers/
â”œâ”€â”€ ğŸ“‚ core/               # Core Infrastructure (v3.0 - Reorganized)
â”‚   â”œâ”€â”€ browser/           # Chrome/Firefox automation, stealth
â”‚   â”œâ”€â”€ config/            # Configuration management
â”‚   â”œâ”€â”€ data/              # Validation, quality checks
â”‚   â”œâ”€â”€ db/                # Database connections (Postgres)
â”‚   â”œâ”€â”€ monitoring/        # Health checks, alerts
â”‚   â”œâ”€â”€ network/           # Proxy, Tor management
â”‚   â”œâ”€â”€ pipeline/          # Orchestration base classes
â”‚   â”œâ”€â”€ progress/          # Tracking & reporting
â”‚   â”œâ”€â”€ reliability/       # Retry logic, rate limiting
â”‚   â””â”€â”€ utils/             # Shared utilities (logging, caching)
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/            # Scraper Implementations
â”‚   â”œâ”€â”€ Argentina/         # Selenium Scrapers
â”‚   â”œâ”€â”€ Malaysia/
â”‚   â”œâ”€â”€ India/             # Scrapy + Wrapper
â”‚   â”œâ”€â”€ Russia/
â”‚   â”œâ”€â”€ Belarus/
â”‚   â”œâ”€â”€ ... (12+ countries)
â”‚
â”œâ”€â”€ ğŸ“‚ gui/                # Desktop GUI Application
â”‚   â”œâ”€â”€ tabs/
â”‚   â”œâ”€â”€ themes/
â”‚   â””â”€â”€ scraper_gui.py     # Main Entry Point
â”‚
â”œâ”€â”€ ğŸ“‚ internal_tools/     # Helper Scripts
â”‚   â”œâ”€â”€ database_setup.py
â”‚   â””â”€â”€ migration_tools.py
â”‚
â”œâ”€â”€ ğŸ“‚ doc/                # Documentation
â”‚   â”œâ”€â”€ PRODUCTION_READINESS_AUDIT.md
â”‚   â”œâ”€â”€ SCRAPER_STATUS_FINAL.md
â”‚   â””â”€â”€ ... (Technical Guides)
â”‚
â”œâ”€â”€ ğŸ“‚ tests/              # Test Suite
â”‚   â”œâ”€â”€ test_production_code.py
â”‚   â””â”€â”€ smoke_test.py
â”‚
â”œâ”€â”€ Dockerfile             # Container definition
â”œâ”€â”€ docker-compose.yml     # Orchestration
â””â”€â”€ requirements.txt       # Python dependencies
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure Environment
Enable the `.env` file with your database credentials:
```bash
cp .env.example .env
# Edit .env with your DB_HOST, DB_USER, etc.
```

### 3. Run a Scraper
```bash
# Standardized entry point for all active scrapers:
python scripts/Argentina/run_pipeline_resume.py --fresh
```

### 4. Launch GUI
```bash
python scraper_gui.py
# or simply double-click run_gui.bat
```

---

## ğŸ“Š Project Status

**Version**: 3.1.0 (Production Ready)
**Date**: Feb 15, 2026

*   âœ… **Core Platform**: 100% Ready (Modular, Secure, Scalable)
*   âœ… **Active Scrapers**: 9 Country Modules (Fully Operational)
*   âš ï¸ **Legacy Scrapers**: 3 Country Modules (Excluded/Broken - see Audit)

For detailed status, read [**PRODUCTION_READINESS_AUDIT.md**](doc/PRODUCTION_READINESS_AUDIT.md).

---

## ğŸ› Key Components

### Core Framework (`core/`)
The heart of the system.
*   **ConfigManager**: Single source of truth for all settings.
*   **SmartRetry**: Intelligent retry logic for network resilience.
*   **ChromeInstanceTracker**: Manages browser processes to prevent leaks.
*   **PipelineCheckpoint**: Ensures robust resume capabilities after crashes.

### Database
*   **PostgreSQL**: Primary data store.
*   **Schemas**: Manage via `core/db/schema_registry.py`.

---

## ğŸ¤ Contributing

1.  **New Scrapers**: Create a new folder in `scripts/CountryName`. Use `run_pipeline_resume.py` as the entry point.
2.  **Core Changes**: Add new modules to the appropriate subdirectory in `core/` (e.g., `core/network/new_proxy.py`).
3.  **Documentation**: Keep `doc/` updated with implementation notes.

---
**Maintained by**: Quad99 Technologies
