#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPPA merge -> FINAL "one brand = one row" table

Input (as generated by your per-row API saving):
- formulationDataTableNew.json                      (list of SKUs with hiddenId etc.)
- per_row_api/001_otherBrandPriceNew.json           (list of other brands)
- per_row_api/001_medDtlsNew.json                   (optional extra fields)
- per_row_api/001_skuMrpNew.json                    (optional extra fields)
- per_row_api/001_row_meta.json                     (optional)

Output:
- final_one_brand_one_row.csv

Rule:
- For each SKU row in formulationDataTableNew => 1 MAIN row
- For each otherBrandPriceNew entry => 1 OTHER row
- Join key everywhere = hiddenId (carried from formulationDataTableNew row)

STATUS FIX:
- Status column MUST come from scheduleStatus (from formulationDataTableNew)

Run:
  python merge_final_one_brand_one_row.py --base "D:/quad99/Scappers/scripts/India/output_nppa/GEMCITABINE"
or point directly to folder where formulationDataTableNew.json exists.
"""

import argparse
import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd


def _load_json(path: Path) -> Any:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _as_list(x: Any) -> List[Any]:
    if x is None:
        return []
    if isinstance(x, list):
        return x
    return [x]


def _safe_get(d: Dict[str, Any], key: str, default: str = "") -> str:
    v = d.get(key, default)
    if v is None:
        return default
    return str(v)


def _parse_index_from_filename(path: Path) -> Optional[int]:
    """
    Accepts:
      001_otherBrandPriceNew.json
      002_medDtlsNew.json
    Returns integer index (1-based) or None.
    """
    m = re.match(r"^(\d+)_", path.name)
    if not m:
        return None
    try:
        return int(m.group(1))
    except Exception:
        return None


def _build_main_row(base_row: Dict[str, Any]) -> Dict[str, Any]:
    """
    One MAIN row per SKU from formulationDataTableNew
    """
    return {
        "HiddenId": _safe_get(base_row, "hiddenId"),
        "BrandType": "MAIN",
        "BrandName": _safe_get(base_row, "skuName"),                # keep full SKU name (matches UI)
        "Company": _safe_get(base_row, "company"),
        "Composition": _safe_get(base_row, "composition"),
        "PackSize": _safe_get(base_row, "packSize"),
        "Unit": _safe_get(base_row, "dosageForm"),
        "Status": _safe_get(base_row, "scheduleStatus"),            # âœ… FIXED (do not use skuName)
        "CeilingPrice": _safe_get(base_row, "ceilingPrice"),
        "MRP": _safe_get(base_row, "mrp"),
        "MRPPerUnit": _safe_get(base_row, "mrpPerUnit"),
        "YearMonth": _safe_get(base_row, "yearMonth"),
    }


def _build_other_row(base_row: Dict[str, Any], sub: Dict[str, Any]) -> Dict[str, Any]:
    """
    One OTHER row per item from otherBrandPriceNew,
    carrying context fields from base_row.
    """
    return {
        "HiddenId": _safe_get(base_row, "hiddenId"),
        "BrandType": "OTHER",
        "BrandName": _safe_get(sub, "brandName"),
        "Company": _safe_get(sub, "company"),
        "Composition": _safe_get(base_row, "composition"),          # carry from main
        "PackSize": _safe_get(sub, "packSize"),
        "Unit": _safe_get(base_row, "dosageForm"),                  # carry from main
        "Status": _safe_get(base_row, "scheduleStatus"),            # carry from main (status of SKU table)
        "CeilingPrice": _safe_get(base_row, "ceilingPrice"),        # carry from main
        "MRP": _safe_get(sub, "brandMrp"),
        "MRPPerUnit": _safe_get(sub, "mrpPerUnit"),
        "YearMonth": _safe_get(sub, "yearMonth") or _safe_get(base_row, "yearMonth"),
    }


def _merge_optional_details(rows: List[Dict[str, Any]],
                            hidden_id: str,
                            sku_mrp_payload: Any,
                            med_dtls_payload: Any) -> None:
    """
    Optional: attach extra fields from skuMrpNew / medDtlsNew if you want.
    This function is conservative: it only adds fields if present.
    It writes into the existing 'rows' dicts in-place.
    """
    # skuMrpNew is usually a list with one dict
    sku0 = _as_list(sku_mrp_payload)
    sku0 = sku0[0] if sku0 else {}

    # medDtlsNew might be dict or list; take dict-like best effort
    med0 = _as_list(med_dtls_payload)
    med0 = med0[0] if med0 else {}
    if not isinstance(med0, dict):
        med0 = {}

    # Pick a few common extra fields (edit if you want more)
    extra = {}
    if isinstance(sku0, dict):
        # keep these only if present
        for k in ["brandName", "composition", "packSize", "company", "mrp", "mrpPerUnit", "yearMonth"]:
            if k in sku0 and sku0[k] is not None:
                extra[f"SKU_{k}"] = str(sku0[k])

    if isinstance(med0, dict):
        # dump med details keys as MED_<key> (only primitive types)
        for k, v in med0.items():
            if v is None:
                continue
            if isinstance(v, (str, int, float, bool)):
                extra[f"MED_{k}"] = str(v)

    if not extra:
        return

    for r in rows:
        if r.get("HiddenId") == hidden_id:
            r.update(extra)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--base", required=True, help="Folder containing formulationDataTableNew.json (and optionally per_row_api/)")
    ap.add_argument("--per_row_dir", default="per_row_api", help="Per-row API folder name inside base (default: per_row_api)")
    ap.add_argument("--out", default="final_one_brand_one_row.csv", help="Output CSV filename (inside base folder unless absolute)")
    ap.add_argument("--dedupe", action="store_true", help="Dedupe rows by (HiddenId, BrandType, BrandName, PackSize)")
    ap.add_argument("--attach_details", action="store_true", help="Attach optional fields from skuMrpNew + medDtlsNew")
    args = ap.parse_args()

    base_dir = Path(args.base).resolve()
    per_row_dir = (base_dir / args.per_row_dir).resolve()
    out_path = Path(args.out)
    if not out_path.is_absolute():
        out_path = base_dir / out_path

    table_path = base_dir / "formulationDataTableNew.json"
    if not table_path.exists():
        raise SystemExit(f"Missing: {table_path}")

    table_data = _load_json(table_path)
    if not isinstance(table_data, list):
        raise SystemExit("formulationDataTableNew.json must be a JSON list")

    # Build quick lookup: hiddenId -> base row
    base_by_hidden: Dict[str, Dict[str, Any]] = {}
    for r in table_data:
        if not isinstance(r, dict):
            continue
        hid = _safe_get(r, "hiddenId").strip()
        if hid:
            base_by_hidden[hid] = r

    # Map per-row files by index (001, 002...) so we can match to table order if needed
    other_files = sorted(per_row_dir.glob("*_otherBrandPriceNew.json"))
    sku_files = sorted(per_row_dir.glob("*_skuMrpNew.json"))
    med_files = sorted(per_row_dir.glob("*_medDtlsNew.json"))

    sku_by_idx = {(_parse_index_from_filename(p) or -1): p for p in sku_files}
    med_by_idx = {(_parse_index_from_filename(p) or -1): p for p in med_files}
    other_by_idx = {(_parse_index_from_filename(p) or -1): p for p in other_files}

    rows: List[Dict[str, Any]] = []

    # Iterate SKUs in table order (1..N)
    for i, base_row in enumerate(table_data, start=1):
        if not isinstance(base_row, dict):
            continue

        hid = _safe_get(base_row, "hiddenId").strip()
        if not hid:
            continue

        # MAIN row
        rows.append(_build_main_row(base_row))

        # OTHER rows (if file exists for this index)
        other_path = other_by_idx.get(i)
        if other_path and other_path.exists():
            other_payload = _load_json(other_path)
            for sub in _as_list(other_payload):
                if isinstance(sub, dict):
                    rows.append(_build_other_row(base_row, sub))

        # Optional details attach
        if args.attach_details:
            sku_payload = _load_json(sku_by_idx[i]) if i in sku_by_idx and sku_by_idx[i].exists() else None
            med_payload = _load_json(med_by_idx[i]) if i in med_by_idx and med_by_idx[i].exists() else None
            _merge_optional_details(rows, hid, sku_payload, med_payload)

    df = pd.DataFrame(rows)

    # Optional dedupe
    if args.dedupe and not df.empty:
        df["_dedupe_key"] = (
            df["HiddenId"].astype(str).fillna("") + "||" +
            df["BrandType"].astype(str).fillna("") + "||" +
            df["BrandName"].astype(str).fillna("") + "||" +
            df["PackSize"].astype(str).fillna("")
        )
        df = df.drop_duplicates(subset=["_dedupe_key"]).drop(columns=["_dedupe_key"])

    # Column order (edit if you want)
    preferred = [
        "HiddenId", "BrandType", "BrandName", "Company",
        "Composition", "PackSize", "Unit", "Status",
        "CeilingPrice", "MRP", "MRPPerUnit", "YearMonth",
    ]
    # keep any extra columns (MED_/SKU_) at the end
    cols = [c for c in preferred if c in df.columns] + [c for c in df.columns if c not in preferred]
    df = df[cols]

    out_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_path, index=False, encoding="utf-8-sig")

    print(f"DONE: {out_path}")
    print(f"Rows: {len(df)} (one brand = one row)")
    # quick sanity: Status should not equal BrandName
    bad = df[(df["Status"].astype(str).str.strip() != "") & (df["Status"].astype(str) == df["BrandName"].astype(str))]
    if len(bad) > 0:
        print(f"[WARN] {len(bad)} rows have Status == BrandName (mapping issue). Check scheduleStatus source.")


if __name__ == "__main__":
    main()
