# Argentina Scraper Documentation

## Overview

The Argentina scraper extracts pharmaceutical product information from AlfaBeta (alfabeta.net), including product lists, pricing, and company details. The scraper processes data through multiple stages: extraction, translation, and final report generation.

## Workflow

The Argentina scraper follows a 7-step pipeline:

1. **00_backup_and_clean.py** - Backup existing output and clean for fresh run
2. **01_getProdList.py** - Extract product list from AlfaBeta
3. **02_prepare_urls.py** - Build product URLs and initialize scrape state
4. **03_alfabeta_selenium_scraper.py** - Scrape detailed product information with Selenium
5. **04_alfabeta_api_scraper.py** - Scrape detailed product information via API (fill gaps)
6. **05_TranslateUsingDictionary.py** - Translate Spanish text to English
7. **06_GenerateOutput.py** - Generate final output report with PCID mapping

## Configuration

All configuration is managed through `config/Argentina.env.json`. The configuration follows the Malaysia format with script-specific prefixes:

- **SCRIPT_00_*** - Backup and clean configuration
- **SCRIPT_01_*** - Product list extraction settings
- **SCRIPT_02_*** - URL preparation settings
- **SCRIPT_03_*** - Selenium scraping configuration (threads, rate limits, login)
- **SCRIPT_04_*** - API scraping configuration (threads, rate limits, API keys)
- **SCRIPT_05_*** - Translation settings
- **SCRIPT_06_*** - Output generation settings

### Key Configuration Values

- `ALFABETA_USER` / `ALFABETA_PASS` - AlfaBeta login credentials (in secrets section)
- `SCRAPINGDOG_API_KEY` - API key for ScrapingDog proxy service
- `HEADLESS` - Run browser in headless mode (default: false)
- `MAX_ROWS` - Maximum number of products to process (default: 100)
- `DEFAULT_THREADS` - Number of concurrent threads (default: 2)
- `RATE_LIMIT_PRODUCTS` / `RATE_LIMIT_SECONDS` - Rate limiting configuration

## Input Files

Place the following files in the input directory (`Input/`):

- `Dictionary.csv` - Spanish to English translation dictionary
- `pcid_Mapping.csv` - PCID mapping file
- `Productlist.csv` - Product list (generated by step 01)
- `ProxyList.txt` - Optional proxy list file

## Output Files

The scraper generates the following output files in the output directory:

- `Productlist.csv` - Extracted product list
- `Productlist_with_urls.csv` - Product list with constructed URLs and scrape state
- `alfabeta_products_by_product.csv` - Scraped product details
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log
- `alfabeta_products_all_dict_en.csv` - Translated product data
- `missing_cells.csv` - Untranslated cells from dictionary translation
- `alfabeta_Report_YYYYMMDD_pcid_mapping.csv` - Final output report with PCID mapping
- `alfabeta_Report_YYYYMMDD_pcid_missing.csv` - Rows without PCID match
- `alfabeta_Report_YYYYMMDD_pcid_oos.csv` - Out-of-scope products
- `alfabeta_Report_YYYYMMDD_pcid_no_data.csv` - Rows missing required data

## Running the Scraper

### Using the GUI

1. Launch `scraper_gui.py`
2. Select "Argentina" from the scraper dropdown
3. Click "Run Pipeline" to execute all steps sequentially

### Using Command Line

Navigate to `scripts/Argentina/` and run:

```batch
run_pipeline.bat
```

Or run individual steps:

```bash
python 00_backup_and_clean.py
python 01_getProdList.py
python 02_prepare_urls.py
python 03_alfabeta_selenium_scraper.py
python 04_alfabeta_api_scraper.py
python 05_TranslateUsingDictionary.py
python 06_GenerateOutput.py
```

## Script Details

### 01_getProdList.py

Extracts product list from AlfaBeta website.

**Input:** None (scrapes from website)
**Output:** `Productlist.csv`

**Configuration:**
- `SCRIPT_01_PRODUCTS_URL` - AlfaBeta products URL
- `SCRIPT_01_HUB_URL` - AlfaBeta hub URL
- `SCRIPT_01_HEADLESS` - Browser headless mode

### 02_prepare_urls.py

Builds product URLs and initializes scrape state.

**Input:** `Productlist.csv`
**Output:** `Productlist_with_urls.csv`

**Features:**
- URL construction based on AlfaBeta patterns
- Initializes scrape state flags for Selenium/API

### 03_alfabeta_selenium_scraper.py

Scrapes detailed product information using Selenium.

**Input:** `Productlist_with_urls.csv`
**Output:**
- `alfabeta_products_by_product.csv` - Product details
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log

**Configuration:**
- `SCRIPT_03_DEFAULT_THREADS` - Number of concurrent threads
- `SCRIPT_03_RATE_LIMIT_PRODUCTS` - Rate limit per product
- `SCRIPT_03_RATE_LIMIT_SECONDS` - Rate limit time window
- `SCRIPT_03_MAX_ROWS` - Maximum rows to process

**Features:**
- Multi-threaded scraping
- Account rotation
- Rate limiting
- Progress tracking
- Error logging

### 04_alfabeta_api_scraper.py

Scrapes detailed product information via API to fill remaining gaps.

**Input:** `Productlist_with_urls.csv`
**Output:**
- `alfabeta_products_by_product.csv` - Product details (updated)
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log

**Configuration:**
- `SCRIPT_04_API_THREADS` - Number of concurrent threads
- `SCRIPT_04_RATE_LIMIT_PRODUCTS` - Rate limit per product
- `SCRIPT_04_RATE_LIMIT_SECONDS` - Rate limit time window
- `SCRIPT_04_SCRAPINGDOG_URL` - ScrapingDog API URL

### 05_TranslateUsingDictionary.py

Translates Spanish text to English using a dictionary file.

**Input:** 
- `alfabeta_products_by_product.csv`
- `Dictionary.csv`

**Output:**
- `alfabeta_products_all_dict_en.csv` - Translated data
- `missing_cells.csv` - Cells that couldn't be translated

### 06_GenerateOutput.py

Generates final output report with PCID mapping.

**Input:**
- `alfabeta_products_all_dict_en.csv`
- `pcid_Mapping.csv`

**Output:**
- `alfabeta_Report_YYYYMMDD_pcid_mapping.csv` - Final report with PCID mapping
- `alfabeta_Report_YYYYMMDD_pcid_missing.csv` - Rows without PCID match
- `alfabeta_Report_YYYYMMDD_pcid_oos.csv` - Out-of-scope products
- `alfabeta_Report_YYYYMMDD_pcid_no_data.csv` - Rows missing required data

**Features:**
- Strict PCID mapping
- Data standardization
- Date-based file naming

## Troubleshooting

### Common Issues

1. **Login Failures**
   - Verify `ALFABETA_USER` and `ALFABETA_PASS` in config
   - Check if credentials are still valid

2. **Rate Limiting**
   - Adjust `RATE_LIMIT_PRODUCTS` and `RATE_LIMIT_SECONDS`
   - Reduce `DEFAULT_THREADS` if getting blocked

3. **Translation Errors**
   - Ensure `Dictionary.csv` is up to date
   - Check `missing_cells.csv` for untranslated entries

4. **PCID Mapping Issues**
   - Verify `pcid_Mapping.csv` format
   - Check column names match expected format

## Dependencies

- Selenium WebDriver
- pandas
- openpyxl (for Excel output)
- Python 3.8+

## Notes

- The scraper uses ScrapingDog for proxy support
- Account rotation is implemented to avoid rate limits
- All configuration values are in `config/Argentina.env.json`
- Secrets (API keys, passwords) are stored in the `secrets` section of the config file

