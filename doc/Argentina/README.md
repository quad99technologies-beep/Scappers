# Argentina Scraper Documentation

## Overview

The Argentina scraper extracts pharmaceutical product information from AlfaBeta (alfabeta.net), including product lists, pricing, and company details. The scraper processes data through multiple stages: extraction, translation, and final report generation.

## Workflow

The Argentina scraper follows a 7-step pipeline:

1. **00_backup_and_clean.py** - Backup existing output and clean for fresh run
2. **01_getProdList.py** - Extract product list from AlfaBeta
3. **02_prepare_urls.py** - Build product URLs and initialize scrape state
4. **03_alfabeta_selenium_scraper.py** - Scrape detailed product information with Selenium
5. **04_alfabeta_api_scraper.py** - Scrape detailed product information via API (fill gaps)
6. **05_TranslateUsingDictionary.py** - Translate Spanish text to English
7. **06_GenerateOutput.py** - Generate final output report with PCID mapping

## Configuration

All configuration is managed through `config/Argentina.env.json`. The configuration follows the Malaysia format with script-specific prefixes:

- **SCRIPT_00_*** - Backup and clean configuration
- **SCRIPT_01_*** - Product list extraction settings
- **SCRIPT_02_*** - URL preparation settings
- **SCRIPT_03_*** - Selenium scraping configuration (threads, rate limits, login)
- **SCRIPT_04_*** - API scraping configuration (threads, rate limits, API keys)
- **SCRIPT_05_*** - Translation settings
- **SCRIPT_06_*** - Output generation settings

### Key Configuration Values

- `ALFABETA_USER` / `ALFABETA_PASS` - AlfaBeta login credentials (in secrets section)
- `SCRAPINGDOG_API_KEY` - API key for ScrapingDog proxy service
- `HEADLESS` - Run browser in headless mode (default: false)
- `MAX_ROWS` - Maximum number of products to process (default: 100)
- `DEFAULT_THREADS` - Number of concurrent threads (default: 2)
- `RATE_LIMIT_PRODUCTS` / `RATE_LIMIT_SECONDS` - Rate limiting configuration

## Input Files

Place the following files in the input directory (`Input/`):

- `Dictionary.csv` - Spanish to English translation dictionary
- `pcid_Mapping.csv` - PCID mapping file
- `Productlist.csv` - Product list (generated by step 01)
- `ProxyList.txt` - Optional proxy list file

## Output Files

The scraper generates the following output files in the output directory:

- `Productlist.csv` - Extracted product list
- `Productlist_with_urls.csv` - Product list with constructed URLs and scrape state
- `alfabeta_products_by_product.csv` - Scraped product details
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log
- `alfabeta_products_all_dict_en.csv` - Translated product data
- `missing_cells.csv` - Untranslated cells from dictionary translation
- `alfabeta_Report_YYYYMMDD_pcid_mapping.csv` - Final output report with PCID mapping
- `alfabeta_Report_YYYYMMDD_pcid_missing.csv` - Rows without PCID match
- `alfabeta_Report_YYYYMMDD_pcid_oos.csv` - Out-of-scope products
- `alfabeta_Report_YYYYMMDD_pcid_no_data.csv` - Rows missing required data

## Running the Scraper

### Using the GUI

1. Launch `scraper_gui.py`
2. Select "Argentina" from the scraper dropdown
3. Click "Run Pipeline" to execute all steps sequentially

### Using Command Line

Navigate to `scripts/Argentina/` and run:

```batch
run_pipeline.bat
```

Or run individual steps:

```bash
python 00_backup_and_clean.py
python 01_getProdList.py
python 02_prepare_urls.py
python 03_alfabeta_selenium_scraper.py
python 04_alfabeta_api_scraper.py
python 05_TranslateUsingDictionary.py
python 06_GenerateOutput.py
```

## Script Details

### 01_getProdList.py

Extracts product list from AlfaBeta website.

**Input:** None (scrapes from website)
**Output:** `Productlist.csv`

**Configuration:**
- `SCRIPT_01_PRODUCTS_URL` - AlfaBeta products URL
- `SCRIPT_01_HUB_URL` - AlfaBeta hub URL
- `SCRIPT_01_HEADLESS` - Browser headless mode

### 02_prepare_urls.py

Builds product URLs and initializes scrape state.

**Input:** `Productlist.csv`
**Output:** `Productlist_with_urls.csv`

**Features:**
- URL construction based on AlfaBeta patterns
- Initializes scrape state flags for Selenium/API

### 03_selenium_3round_wrapper.py

Scrapes detailed product information using Selenium with 3-round retry mechanism.

**Input:** `Productlist_with_urls.csv`
**Output:**
- `alfabeta_products_by_product.csv` - Product details
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log

**Configuration:**
- `SELENIUM_ROUNDS` - Number of retry rounds (default: 3)
- `ROUND_PAUSE_SECONDS` - Pause between rounds (default: 60 seconds)
- `SELENIUM_THREADS` - Number of concurrent threads (default: 4)
- `RATE_LIMIT_PRODUCTS` - Rate limit per product
- `RATE_LIMIT_SECONDS` - Rate limit time window
- `MAX_ROWS` - Maximum rows to process per round

**Features:**
- **3-Round Retry Mechanism:**
  - **Round 1:** Process all products marked for Selenium scraping
  - **Round 2:** Retry products that returned 0 records in Round 1
  - **Round 3:** Final retry for products still with 0 records after Round 2
- Multi-threaded scraping within each round
- Account rotation
- Rate limiting
- Progress tracking per round
- Error logging
- Automatic marking of failed products for API scraping (if enabled)

**How It Works:**

1. **Round 1** processes all products with `Scraped_By_Selenium=no`
2. After Round 1, the script identifies products with `Selenium_Records=0`
3. **Round 2** only processes those products that failed in Round 1
4. After Round 2, the script again identifies remaining failures
5. **Round 3** gives one final attempt to products that still have no data
6. After Round 3, if `USE_API_STEPS=True`, failed products are marked with `Source=api` for API scraping

**Attempt Tracking:**

The wrapper maintains two tracking columns in `Productlist_with_urls.csv`:
- `Selenium_Attempt`: Current attempt number (0-3)
- `Last_Attempt_Records`: Number of records found in the last attempt

This allows the pipeline to:
- Skip successfully scraped products in subsequent rounds
- Identify persistent failures after all 3 attempts
- Generate statistics on retry effectiveness

**Why 3 Rounds?**

Many scraping failures are temporary:
- **Network issues:** Temporary connection problems or timeouts
- **Rate limiting:** Website temporarily blocking due to too many requests
- **Dynamic content:** JavaScript-heavy pages that occasionally fail to load
- **Server load:** Website under heavy load returning empty responses
- **Session issues:** Cookie/session expiration during long scraping runs

The 3-round approach significantly improves data coverage:
- **Round 1:** Typically succeeds for 70-80% of products
- **Round 2:** Recovers an additional 15-20% that failed temporarily
- **Round 3:** Recovers final 5-10% with one last attempt
- **Remaining failures:** True issues (product not found, discontinued, etc.) â†’ sent to API

**Best Practices:**

- Let all 3 rounds complete before evaluating results
- Monitor the "Retry Effectiveness" stats in the final summary
- If Round 1 success rate is very low (<50%), check your config/credentials
- Adjust `ROUND_PAUSE_SECONDS` if you're hitting rate limits (increase to 90-120s)
- Products that fail all 3 rounds likely have data issues (check manually)

### 04_alfabeta_api_scraper.py

Scrapes detailed product information via API to fill remaining gaps.

**Input:** `Productlist_with_urls.csv`
**Output:**
- `alfabeta_products_by_product.csv` - Product details (updated)
- `alfabeta_progress.csv` - Progress tracking
- `alfabeta_errors.csv` - Error log

**Configuration:**
- `SCRIPT_04_API_THREADS` - Number of concurrent threads
- `SCRIPT_04_RATE_LIMIT_PRODUCTS` - Rate limit per product
- `SCRIPT_04_RATE_LIMIT_SECONDS` - Rate limit time window
- `SCRIPT_04_SCRAPINGDOG_URL` - ScrapingDog API URL

### 05_TranslateUsingDictionary.py

Translates Spanish text to English using a dictionary file.

**Input:** 
- `alfabeta_products_by_product.csv`
- `Dictionary.csv`

**Output:**
- `alfabeta_products_all_dict_en.csv` - Translated data
- `missing_cells.csv` - Cells that couldn't be translated

### 06_GenerateOutput.py

Generates final output report with PCID mapping.

**Input:**
- `alfabeta_products_all_dict_en.csv`
- `pcid_Mapping.csv`

**Output:**
- `alfabeta_Report_YYYYMMDD_pcid_mapping.csv` - Final report with PCID mapping
- `alfabeta_Report_YYYYMMDD_pcid_missing.csv` - Rows without PCID match
- `alfabeta_Report_YYYYMMDD_pcid_oos.csv` - Out-of-scope products
- `alfabeta_Report_YYYYMMDD_pcid_no_data.csv` - Rows missing required data

**Features:**
- Strict PCID mapping
- Data standardization
- Date-based file naming

## Troubleshooting

### Common Issues

1. **Login Failures**
   - Verify `ALFABETA_USER` and `ALFABETA_PASS` in config
   - Check if credentials are still valid

2. **Rate Limiting**
   - Adjust `RATE_LIMIT_PRODUCTS` and `RATE_LIMIT_SECONDS`
   - Reduce `DEFAULT_THREADS` if getting blocked

3. **Translation Errors**
   - Ensure `Dictionary.csv` is up to date
   - Check `missing_cells.csv` for untranslated entries

4. **PCID Mapping Issues**
   - Verify `pcid_Mapping.csv` format
   - Check column names match expected format

## Dependencies

- Selenium WebDriver
- pandas
- openpyxl (for Excel output)
- Python 3.8+

## Notes

- The scraper uses ScrapingDog for proxy support
- Account rotation is implemented to avoid rate limits
- All configuration values are in `config/Argentina.env.json`
- Secrets (API keys, passwords) are stored in the `secrets` section of the config file

## Health Check (Manual)

- Run `python scripts/Argentina/health_check.py` before starting a new scrape to exercise the
  configuration logic, PCID file readiness, and the AlfaBeta selectors required to submit the
  blank search form and render product results.
- The script prints a matrix with `PASS/FAIL` for each step and writes a timestamped copy to
  `exports/Argentina/health_check/health_check_<timestamp>.txt`.
- Its checks include:
  1. HTTP reachability of `PRODUCTS_URL`.
  2. Presence of `pcid_Mapping.csv` under the input directory.
  3. Search form selectors (`form#srvPr`, `input[name='patron']`, `input.mfsubmit`).
  4. Product listing hints (`table.estandar`, links with `a.rprod`).
- Because it only fetches layout data, the health check never writes output files or alters data.
- The GUI also exposes a **Health Check** tab that runs the same script so you can fire it from
  the interface, watch the matrix log stream, and make sure the selectors/configuration are intact
  before pressing any pipeline buttons.

